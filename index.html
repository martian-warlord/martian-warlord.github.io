<h1>DGMD S-15 Class Journal</h1>

<h2><a href="https://gist.github.com/martian-warlord/99f887919db36c937007">People page Project</a></h2>
<h4>Project description: Customize a digital business card using HTML, JS, and CSS.</h4>
<p>For this project I started by looking for a halfway decent picture of myself.  For the past couple of years I've been very focused on being a new parent, so all my pictures are on my son and my wife.  While I was laughing about this problem on the phone with my wife, she suggested a picture where I was bouncing my son up and down.  I had a natural smile, and my son was happy and had a cool blur effect from the exposure.  I thought it was a great suggestion.  Fortunately, this was right before my computer crashed and I had to erase all my data, so I was able to find the pix.  However, the pictures were too personal.  The blur was a blur.  If you didn't know it was my son John, it was a blur. It occured to me that the series would make a good slide show or animated gif.  If all the pictures were shown as a sequence, the viewer would see the blur and see John.  I looked at the example code provided to class and saw two directions that could work.  One example would pull random photos from a repository.  I figured that I could create a timer and loop the request.  The second example used a sprite sheet.  I had always wanted to do a sprite sheet, so I chose that direction.</p>
<p>I was new to the Mac's Photos app, and discovered that they had added an export step to using their photo album directory.  The interface didnt seem to have a step where you chose an end directory for the export, and I didnt want to dig around in the preferences.  So I pulled up the larger view of the photos in the album, and I screen grabbed them.  I then imported the photos into Photoshop.  I created a script to crop and resize one of the photos, and applied it to all of them.  This gave them a uniform dimension.  I then created a canvas for a sprite sheet and dropped them all in.  I saved the sprite sheet for the web and it was only 26k or so.  I wasnt going to be a memory hog.  Yay!</p>
<p>I then thought about attaching the sprite effect to a mouse over event.  My initial problem was that my images didn't work as a still.  If some one looked my card from a mobile device they wouldnt be able to hover.  So, I decided to start the animation on load.  It was less interactive within the context of "mouse interaction" but it's emotional cinematic narrative value was very engaging.  I had initially included a color scrolling effect on the background color but I decided that I already had a strong visual and some times less is more.  </p>

<h2><a href="https://github.com/martian-warlord/a-story-in-pomes">Story Project</a></h2>
<h4>Project description: Build a digital narrative, using a map/poetry/camera-poem-picker template as a jumping off point.</h4>
<p>I started this project by thinking of two directions.  I love computational poetry.  So I wanted to make poetry machines instead of using static poems.  I also wanted to use word clouds.  Last semester I took a class in data visualization and I was introduced to various text visualization areas.  I love concrete poetry, and Mallarmé's fin de siècle style where poetry is mixed with more spatial arts. </p>
<p>The next part of this project's development worked along two lines.  One line of thought progressed from the word clouds the second progressed from the map api that was provided with the project.</p>
<p>As I thought about word clouds, I started pulling vocabulary words from my standard bag of tricks. I have worked in the area of poetry and automatic writing for a long time, so I have an existing style and body of work that has been my anchor.  It's very easy for me to drop into my comedy/horror/satire work flow. I was thinking about this as I started playing with the map API</p> 
<p>I started customizing the sory-in-pomes template by zooming Googles map API into Fez, Morocco where the streets are so twisted they didnt even publish maps until recently.  It looked great. Map based narrative has recently been more rooted in the video game narrative domain so I started thinking about video game narratives.  One dimesion of game playing a narrative is the use of prescriptive story events rather than descriptive events.</p> 
<p>While I was very happy with the first step of the map based horror/comedy/satire narrative game direction, I reminded myself that some of my better job leads in Madison Wisconsin are doing educational games.  If I wanted to extend the life of this project beyond class goals, I probably wanted to make something fun happy and silly.  I had previous scraped a list of vocabulary words from a wide array of self affirmation posts on the internet.  I decided approach my first word cloud with this list.</p>
<p>To implement the word cloud I started by looking for an example that could generate word clouds dynamically from arrays.  I wanted an approach that could be combined with generative poetry.  I found a nice D3 example, and then found a stripped down version that only used right angles.  I dropped in the list of affirmations and it looked awesome.</p>
<p>Back at the map based approach, I decided that Google Maps wasnt as relevant to this project but that the affirmations worked as an affective map or emotional map.  I thought it would make a good compass pointer for quests.  At this point I was at a cross roads.  The introduction ofthe template as part of the class assignment was suggestive of an assigned approach.  However, a previous conversation I had had with Alec on the subject of class parameters seemed to be open to exploration.  I decided to move forward and double check with Alec.  "Yes" it's a safe assumption that this conceptual jump works with the class's educational goals.  </p>
<p>So the project, at it's core became clicking on word cloud key events and triggering quests from a list that matches the appropriate keyword.  I started thinking about how to export the word array.  It seemed more direct to get this list after it had been propogated to the screen as svg text.  I searched around athe D3 documentation and found the right syntax to add corresponding IDs to each of the text words.  Upon reflection this wasnt necessarily the right approach but I kept it incase I wanted to add animation to a click event.  The next direction was to use D3 click handling syntax to trigger a function that corresponds to each text word. I spent several hours searching for the right documentation to accomplish this direction.  Eventually I asked Alec and he told me about the eval() function to get the string value of an array call. It worked!  I was now triggering corresponding keyword functions based on the keyword click events.  </p>
<p>I then thought about how to display the quest text.  We had just worked on a personal pages with a card flipping effect.  It seemed well suited to a small screen app.  However, I remember the card structure to be a bit convoluted so I searched for a more basic tutorial online.  I looked at five or six examples, asked Shaunalynn for help, and cobbed together a version that worked.</p>


<h2><a href="https://github.com/martian-warlord/DGMDS15">Final Project</a></h2>
<h4>Project description: Creative Exploration in Screen-Based and Physical Computing.</h4>
<h3>SMILE!</h3>
<p>Description: An app that takes a picture, when it detects a smile.</p>
<p>Concept: Police have a PR problem where a lot of messier customer contacts have been publicized.  One of the critiques of police is that they are not emotionally sensitive in their work.  This app aims to gamify police work and reinforce positive police contacts.  It uses a body cam to take a picture whenever anyone smiles.  Smile pictures can reinforce positive police self image.  Counts of smile pictures can be used in competition in order to encourage positive emotional interaction between police and customers.</p>
<p>Prototype details: This project will use JS Node and a go pro camera to connect to a face/smile recognition API.  When a smile is detected it will save a picture to a smart phone.
<ul>
<li>Target User: Police with body cams</li>
<li>Problem it solves: Emotional Awareness and PR</li>
<li>Hardware list: Body cam and Harness, smart phone </li></ul>

<h4>Production notes</h4>
<p>On the second day of class I went to Best Buy to get the most popular body camera and smart watch.  It looks like body cameras have not caught on at all.  CV capabilities ahve not been successfully turned into mobile products and people don't have a reason to see the upside of these wearables vs concerns or indifference.  So...  The GoPro is an "Action Camera."  This means that its big and (proabbly due to spy cam/bad publicity concerns) extremely conspicuous. The only harness for wearing the camera on the body looks like a military harness.  </p>

<p>The most popular smart watch is the fit bit.  I purchased a cheaper fitbit expecting it to have a pulse tracking ability I could use to trigger the camera.  No.  Its basically a $100 pedometer.  I return the next day and get the more expensive model that measures pulse.  I do research into existing projects that use pulse measurement.  It looks like the pulse API is only available by special request to Fitbits developer team.  It seems that fitbit is not a good interface for immediate events.  It expects its moment to moment readings to be be iffy and thus uses averages to get reasonable numbers.  It uses progressive enhancement as a marketing tool.  The more expensive Fitbits seem may have the ability to interact with immediate events, but I have to set aside that path for another project.  Right now Fitbit is a bust.  I should have bought an Apple Watch.  I continue to use the Fitbit to learn more about its UX.  It looks like its main interaction with the customer is to push positive encouraging messages over and over again.  I gues customers are less likely to drop a product that sends several compliments per day.  </p>

<p>Back to the gopro. </p>

<p>I forgot thi step, so I'm going back and adding it.  Initially, I wanted use GoPro as a video camera where the user stored a buffer to a cellphone. The user would then use triggers to offload the buffer as a file.  When I discussed this with Alec he suggested looking for streaming documentation.  My initial GoPro research was into streaming options.  There was an extremely convoluted description of connecting GoPro to GoLive.  Hmmmm.   Later conversation with Alec leads to a decision to merely taking a picture and saving it to a local smart phone or computer.  When I purchase the GoPro I discover that the set up process is stupid.  The camera only has two buttons and the documentation keeps changing the words it uses to describe these same buttons.  I end up having more success when I ignore the official documentation and use the device intuitively.  The most stressful part is putting in the memory card, which has to be shoved in hard at an angle.  A first time user might not want to shove parts in at weird angles, without knowing they are doing the right thing first.  Now the camera is working.</p>

<p>Alec shows me a Node library with a nice set of functions.  I can only wrap my brain around node by relating it to complete web applications.  This is incorrect.  I should look at it more in relation to pre-processors.  Except that I have to install node in the directory on the server.  ...  And I have to use Bower if I want the browser to use these scripts.  Um...  I need a class in this stuff.  I watch every tutorial on Linda.  I get a basic understanding of dependency lists and installing for the comand line versus for a given project and etc.  I start to dig in.  No the learning curve is not easy for me.  I go back and start walking through the easier tutorials step by step.  Sam tells me about Meteor.  I start a Meteor tutorial.  I start to tell Alec and he gives me a bewildering look and shows that I should merely install the node package in a directory and use it like a preprocessor.  </p>
<p>Meanwhile I have been digging into the node library documentation to see if I can figure out how the functions work and bypass the node library.  I randomly pull open piece of side documentation and find a list of URL commands.  I think that the link are to further documentation but Im getting JSON back instead.  Hmmmm.  URLs as commands?  Thats new to me. I follow the instructions used for syncing GoPro's local Wifi network to the iPhone app and connect my computer. Yes, it recognizes the camera and I can start the video.  Unfortunately theres no description for taking a picture.  I spend a couple hours Googling around for anything about taking a picture.  Eventually I get the idea to put the camera into photo mode and trigger the shutter.  Bullseye.  It worked.  

I decided to put them in a test html file.  I assume that they need error handling so I use the AJAX example that I was using for Flickr.  (I think its half based on the AJAX example Alec made as a Pome option?).  I follow 





<h2>DAILY THOUGHTS</h2>
<h3>Week 1</h3>
<h4>DAY 1</h4>
Getting situated
Why do people code?
specifying and scoping great projects
 


<p>Met w Alec at Clover</p>
<p>Tech Hell</p>
<p>Personal Page.  Intro to HTML, CSS, Javascript</p>
<h4>DAY 2</h4>
pomes, /people, and github
resources on stories and storytelling

<p>Tech Fixed.  Intro to Pomes.  Buying Wearables.</p>
<h4>DAY 3</h4>
what makes for good stories


<h4>DAY 4</h4>
short overview of the tessel’s capabilities
 
<h4>DAY 5</h4>
<p>Flickr API.  Scaled back project expectations.</p>

<h3>Week 2</h3>
<h4>DAY 1</h4>
stepping-stone sketches
spec-ing and sketching

<p>Project Discussion.  Project goals submitted. 10 Tuts.  Brick wall w Node.  Trying to learn node.</p>
<h4>DAY 2</h4>
Circles, object orientation, pair programming
<p>Paired Programming.  Circles.  Project breakthrough w URL approach.</p>
The lecture was a nice demo of paired programming and object oriented graphics.

I was at a front table with two beginners and another student with intermediate skills.  We decided to pair by skills.  It was cool to work with Joe.  

<h4>DAY 3</h4>
Circles, object orientation, pair programming, and project time.

Today we continued working on our projects using paired programming.  

I paired up with Joe again and we reviewed the work we had done yesterday.  I ad to run out to meet with a thesis advisor but Joe had continued on and taken the animation to a nice conclusion.

We started talking about his project.  He was doing an animation of the life of a giant star.  He was starting with a nebula and was interested in a CSS cloud animation I had on my web site.  He had found the tutorial that the animation was based on.  I explained how it worked and we discussed how it could be modified to suit his purposes.  I had previously done a nebular version of the cloud animation so I shared what I had learned about the animations strengths and weaknesses.  

Next we discussed my GoPro project.  I explained that I had discovered the URL approach to controlling the camera. We discussed asynchronous http url calls and whether AJAX was the right approach.  Joe had an Ajax example he had used on a previous project but it was jQuery.  Joe was excited to work on his clouds, and I had a good direction with the GoPro so we split off to work on our independent projects.

<h4>DAY 4</h4>
Thursday was a day for demos.  It was good to interact with the other students about their passion projects.  There were some cool topics discussed such as the design of information aggregation tools for wikis and AR games.  At this point I thought that I had to learn node to get beyond file manipulation constraints.

I had just learned that the smile detection API was a dead end and that I would need to find a new direction.  I thought that uploading a captured image to Flickr would be a reasonably achievable direction but hadnt found the right documentation to make the process clear.  I was watching a lot of node tutorials.

<h4>DAY 5</h4>
This was a production day.  Alec emphasized that Node didnt need to be framed as a web app to be useful.  However, the Node packages he used did have the limitation that I worried about.  Fortunately, he found JS file handling library and a trick to bounce a file from the screen to a download.  He also seemed to find a Flickr API tutorial that showed the same technique for uploading a file.  Unfortunately, when I tried to implement this later I discovered that the Flickr API had been updated and didnt match the technical recipe.  After working on this for several hours I set it aside.  

Kamran had sent over a copy of his Um detector so I could see how he manipulated the Pocket Sphynx demo.  I played around with the demo and limited the word list to one trigger word.  It seemed extremely open to mis-interpretting noise.  Searching for further information I found an easy to use API for speech detection called AnnYang.  Encouraged by this find, I searched further and found a voice to speech API.  I found a few more emotion recogntion APIs but the best match had a low thresh hold of calls before triggering a payment.  Searching around I returned to an open source github project for face recognition it had a couple interesting directions. 

<h4>DAY 6</h4>
This was a big breakthrough day for me.  I added full speech recognition and speech synthesis to my pomes project.  Its a bit buggy but works.  The only snag was the API losing its American Male voice in the middle of my implementation.  It started throwing errors and I spent two hours looking for a bug before discovering it was the API.  

I then played with the face detection project for the rest of the day.  I was able to recreate the face detection demo.  I was able to recreate a demo for pulling emotional data from a video stream.  I spent most of my time trying to figure out how to turn the video stream into a frame grab.  I read a lot of documentation on video and canvas.  The emotion recognition project had very little documentation.  The Face detection documentation kept referencing academic papers that seemed to be behind a pay wall.  Eventually I figured out how to manipulate the emotion tracking data and then plug those functions into the face recognition loop.  I had smile detection.  It was nice to get past a four day roadblock!  :-)

 


